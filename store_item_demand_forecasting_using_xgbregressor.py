# -*- coding: utf-8 -*-
"""Store Item Demand Forecasting Using XGBRegressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tyLQy_ky2GEGvn_fa-wOF6G1BjoFJFDx
"""

import pandas as pd
from google.colab import files
import numpy as np
import matplotlib.pyplot as plt
import datetime
from sklearn.model_selection import train_test_split, TimeSeriesSplit
import xgboost as xgb
from sklearn.metrics import mean_absolute_error

train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

# Define SMAPE (Symmetric Mean Absolute Percentage Error) function
def smape(A, F):
    return 100 / len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))

# Display column names and shapes
print("Train columns:", train.columns)

print("Test columns:", test.columns)

print("Train shape:", train.shape, "Test shape:", test.shape)

print('Train store values:', train['store'].unique())

print('Train item values:', train['item'].unique())

print('Test store values:', test['store'].unique())

print('Test item values:', test['item'].unique())

# Visualize the seasonality in sales data
plt.figure(figsize=(12, 8))
plt.subplot(3, 1, 1)
plt.plot(train['sales'][:365 * 4])
plt.title('Sales over 4 years')

plt.subplot(3, 1, 2)
plt.plot(train['sales'][:365])
plt.title('Sales over 1 year')

plt.subplot(3, 1, 3)
plt.plot(train['sales'][:31])
plt.title('Sales over 1 month')
plt.tight_layout()
plt.show()

# Calculate and visualize rolling mean
rolling_mean = train['sales'].rolling(window=7).mean()

plt.figure(figsize=(12, 8))
plt.subplot(3, 1, 1)
plt.plot(rolling_mean[:365 * 4])
plt.title('7-day Rolling Mean over 4 years')

plt.subplot(3, 1, 2)
plt.plot(rolling_mean[:365])
plt.title('7-day Rolling Mean over 1 year')

plt.subplot(3, 1, 3)
plt.plot(rolling_mean[:31])
plt.title('7-day Rolling Mean over 1 month')
plt.tight_layout()
plt.show()

# Calculate 30-day rolling mean
rolling_mean2 = train['sales'].rolling(window=30).mean()

plt.figure(figsize=(10, 6))
plt.subplot(2, 1, 1)
plt.plot(rolling_mean2[:365 * 4])
plt.title('30-day Rolling Mean over 4 years')

plt.subplot(2, 1, 2)
plt.plot(rolling_mean2[:365])
plt.title('30-day Rolling Mean over 1 year')
plt.tight_layout()
plt.show()

# Combine train and test data for feature engineering
data_combine = pd.concat([train, test])

# Convert 'date' column to datetime and create additional features
data_combine['date'] = pd.to_datetime(data_combine['date'], infer_datetime_format=True)
data_combine['month'] = data_combine['date'].dt.month
data_combine['weekday'] = data_combine['date'].dt.dayofweek
data_combine['year'] = data_combine['date'].dt.year
data_combine['week_of_year'] = data_combine['date'].dt.isocalendar().week
data_combine['date_order'] = (data_combine['date'] - datetime.datetime(2013, 1, 1)).dt.days

# Feature engineering: rolling mean and shifts
data_combine['sale_moving_average_7days'] = data_combine.groupby(["item", "store"])['sales'].transform(lambda x: x.rolling(window=7, min_periods=1).mean())
data_combine['sale_moving_average_7days_shifted-90'] = data_combine.groupby(["item", "store"])['sale_moving_average_7days'].transform(lambda x: x.shift(90))
data_combine['store_item_shifted-90'] = data_combine.groupby(["item", "store"])['sales'].transform(lambda x: x.shift(90))
data_combine['store_item_shifted-10'] = data_combine.groupby(["item", "store"])['sales'].transform(lambda x: x.shift(10))

# Splitting data back into train and test sets
train_new = data_combine.loc[~data_combine.sales.isna()]
test_new = data_combine.loc[data_combine.sales.isna()]

# Define features and target
col = [i for i in data_combine.columns if i not in ['date', 'id', 'sales', 'sale_moving_average_7days']]
X_train, X_test, y_train, y_test = train_test_split(train_new[col], train_new['sales'], test_size=0.15, random_state=42)

# Train multiple XGBoost models with varying max_depth
model_sets = []
for max_depth in range(4, 17, 3):
    xgb_model = xgb.XGBRegressor(max_depth=max_depth, min_child_weight=1, eval_metric=smape)
    xgb_model.fit(X_train, y_train)
    model_sets.append(xgb_model)

    y_train_pred_xgb = xgb_model.predict(X_train)
    y_test_pred_xgb = xgb_model.predict(X_test)
    print('SMAPE error: max_depth=', max_depth, ', train:', smape(y_train, y_train_pred_xgb), 'test:', smape(y_test, y_test_pred_xgb))
    print('MAE train:', mean_absolute_error(np.log1p(y_train), np.log1p(y_train_pred_xgb)), 'test:', mean_absolute_error(np.log1p(y_test), np.log1p(y_test_pred_xgb)))

# Choose the best-performing model (in this example, model_sets[2])
best_model = model_sets[2]
best_model.fit(train_new[col], train_new['sales'])

y_train_pred_xgb = best_model.predict(X_train)
y_test_pred_xgb = best_model.predict(X_test)
print('Final SMAPE error: train:', smape(y_train, y_train_pred_xgb), 'test:', smape(y_test, y_test_pred_xgb))
print('Final MAE train:', mean_absolute_error(np.log1p(y_train), np.log1p(y_train_pred_xgb)), 'test:', mean_absolute_error(np.log1p(y_test), np.log1p(y_test_pred_xgb)))

# Predict on test data and prepare submission file
y_submission = np.rint(best_model.predict(test_new[col]))
final = pd.DataFrame(list(zip(test_new['id'], y_submission)), columns=['id', 'sales'])
final['id'] = final['id'].astype(int)
final['sales'] = final['sales'].astype(int)

# Save submission to CSV
final.to_csv("submission.csv", sep=',', index=False)

